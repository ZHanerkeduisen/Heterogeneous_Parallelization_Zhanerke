Zhanerke Duisen, ADA - 2403M

# Practice 7

Блок-схемы в файле "БЛОК-СХЕМЫ"

Ответы на контрольные вопросы в файле "Контрольные вопросы practice7.docx"

Описание:
Изучение и реализация параллельных алгоритмов редукции и сканирования (префиксной суммы) на GPU с использованием CUDA. Исследование производительности этих алгоритмов и их оптимизация.

Реализовать параллельный алгоритм префиксной суммы (scan) по методу Blelloch Scan на GPU с использованием технологии CUDA, обеспечив корректную обработку массивов больших размеров и сравнение производительности с CPU.
________________________________________________________________________________________________________________________________________________________________

# Задание 1: Реализация редукции (Practice7_Task1.ipynb)

Цель: Реализовать алгоритм суммирования элементов массива на GPU с использованием разделяемой памяти CUDA.

Описание:

 - Используется ядро reductionKernel, которое выполняет редукцию внутри блока.

 - Каждый поток загружает элемент массива в shared memory.

 - Пошаговое суммирование с синхронизацией потоков внутри блока.

 - Сумма каждого блока записывается в глобальную память.

 - Финальная сумма блоков суммируется на CPU.

Реализации:

 - Используется shared memory для ускорения доступа к данным.

 - Количество потоков на блок — 256.

 - Масштабируется на массивы любого размера (тесты: 1024, 1 000 000, 10 000 000 элементов).

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile practice71.cu
[код программы]

 Компилируем
!nvcc practice71.cu -o practice71 -arch=sm_75 -std=c++11    

 Запускаем
!./practice71

<img width="655" height="638" alt="image" src="https://github.com/user-attachments/assets/a064836d-0837-4b2d-a190-6d2f43b8872d" />

# Анализ результатов:

Для массива из 1024 элементов CPU выполнил вычисление за 0.0027 мс, тогда как GPU затратил 0.10752 мс. Это связано с тем, что при малых объёмах данных накладные расходы на запуск CUDA-ядра и передачу данных превышают выигрыш от параллельных вычислений.

При размере массива 1 000 000 элементов GPU выполнил редукцию за 0.14976 мс, тогда как CPU потребовалось 2.56092 мс. Таким образом, GPU обеспечил значительное ускорение благодаря одновременной работе большого числа потоков.

На массиве из 10 000 000 элементов GPU завершил вычисления за 1.21094 мс, в то время как CPU затратил 25.9017 мс. Это показывает, что эффективность GPU возрастает с увеличением размера входных данных.

Сравнение CPU и GPU:
CPU выполняет редукцию последовательно, поэтому время выполнения линейно растёт с увеличением размера массива. GPU, напротив, использует сотни параллельных потоков, что позволяет существенно сократить время обработки больших массивов. Однако при малых размерах данных использование GPU нецелесообразно из-за накладных расходов.

Рекомендации по оптимизации:
Для повышения производительности рекомендуется использовать GPU только при достаточно больших объёмах данных. Следует активно применять разделяемую память внутри CUDA-блоков для ускорения доступа к данным, подбирать оптимальный размер блока потоков и минимизировать пересылку данных между CPU и GPU. Дополнительно производительность можно повысить за счёт многоуровневой редукции и уменьшения числа запусков CUDA-ядер.

_______________________________________________________________________________________________________________________________________________________________

# Задание 2: Реализация префиксной суммы (Practice7_Task2.ipynb)

Цель: Реализовать алгоритм префиксной суммы на GPU.

Описание:

 - Используется ядро prefixSumKernel.

 - Для каждого блока выполняется двухшаговое сканирование:

 - Upsweep (шаг вверх) — строим дерево сумм.

 - Downsweep (шаг вниз) — вычисляем префиксные суммы.

 - Используется shared memory для ускорения вычислений.

 - На выходе GPU формируется массив, содержащий префиксные суммы элементов.

Реализации:

 - CPU версия для сравнения производительности выполняется последовательно.

 - Выводятся только времена выполнения CPU и GPU.

 - Используется один блок для упрощённой демонстрации.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile practice72.cu
[код программы]

 Компилируем
!nvcc practice72.cu -o practice72 -arch=sm_75 -std=c++11    

 Запускаем
!./practice72

<img width="1191" height="542" alt="image" src="https://github.com/user-attachments/assets/34aaf77f-dd96-4d20-acaf-748187cee9d7" />

# Анализ результатов:

Была реализована префиксная сумма (сканирование) на CPU и GPU с использованием параллельного алгоритма Blelloch Scan.

Для массива из 1024 элементов CPU справился за 0.003083 мс, а GPU потребовалось 0.1416 мс. При малом объёме данных GPU уступает CPU, так как накладные расходы на запуск CUDA-ядра и синхронизацию потоков превышают выигрыш от параллельной обработки.

На массиве из 1 000 000 элементов GPU показал 0.413888 мс против 4.7016 мс у CPU. Это демонстрирует значительное ускорение за счёт одновременной обработки множества элементов потоками GPU.

Для массива из 10 000 000 элементов GPU обработал данные за 3.98112 мс, тогда как CPU затратил 44.8747 мс, что подтверждает эффективность параллельного алгоритма на больших объёмах данных.

Сравнение CPU и GPU:

CPU выполняет сканирование последовательно, поэтому время растёт линейно с увеличением размера массива. GPU использует параллельные блоки и разделяемую память для ускорения вычислений, что обеспечивает резкое сокращение времени обработки на больших массивах.

При малых массивах GPU проигрывает CPU из-за накладных расходов на инициализацию потоков и синхронизацию.

Рекомендации по оптимизации:

Для максимальной производительности следует:

Использовать GPU для больших массивов.

Применять разделяемую память внутри блоков для ускорения операций суммирования.

Подбирать оптимальный размер блока потоков для конкретного устройства.

Применять многоуровневый или рекурсивный Blelloch Scan для массивов, превышающих размер блока.

________________________________________________________________________________________________________________________________________________

# Задание 3: Анализ производительности (Practice7_Task3.ipynb)

Цель: Сравнить производительность CPU и GPU для редукции и сканирования.

Описание:

 - Для массивов разных размеров (1024, 1 000 000, 10 000 000) измеряется время выполнения:

 - Редукции на CPU и GPU

 - Сканирования на CPU и GPU

 - Используются CUDA-события для точного измерения времени GPU.

 - CPU вычисления выполняются с помощью стандартного последовательного цикла.

Реализации:

 - Подчёркивается параллельность GPU, где потоки блока выполняют редукцию и сканирование одновременно.

 - Использование shared memory значительно ускоряет доступ к данным внутри блока.

 - После завершения вычислений выполняется освобождение памяти GPU и CPU.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile practice73.cu
[код программы]

 Компилируем
!nvcc practice73.cu -o practice73 -arch=sm_75 -std=c++11    

 Запускаем
!./practice73

<img width="718" height="579" alt="image" src="https://github.com/user-attachments/assets/8fa22f13-8f92-4d5b-aa21-22020df3fb27" />

# Анализ результатов:

В рамках задания была проведена оценка производительности операций редукции и сканирования на CPU и GPU для массивов различного размера.

Для массива из 1024 элементов CPU превосходит GPU по редукции (0.003553 мс против 0.153696 мс), что объясняется накладными расходами на запуск CUDA и синхронизацию потоков. GPU показывает преимущество даже в сканировании (0.028576 мс против 0.004041 мс) — благодаря параллельной обработке блоков.

На массивах в 1 000 000 элементов GPU значительно быстрее CPU для обеих операций: редукция 0.140608 мс против 2.88083 мс, сканирование 0.321344 мс против 5.14558 мс. Это демонстрирует эффект масштабируемой параллельной обработки.

Для массива в 10 000 000 элементов GPU сохраняет высокую эффективность: редукция 1.26176 мс против 27.0933 мс у CPU, сканирование 3.04918 мс против 47.2609 мс. GPU обрабатывает большие массивы почти в 20 раз быстрее CPU.

Сравнение CPU и GPU:

 - CPU работает последовательно, поэтому время растёт линейно с размером массива.

 - GPU использует параллельные потоки и разделяемую память, что позволяет одновременно обрабатывать большое количество элементов.

 - Для малых массивов накладные расходы GPU выше, чем выигрыш от параллельной обработки.

 - С ростом объема данных GPU проявляет значительное преимущество.

Рекомендации по оптимизации

 - Для больших массивов использовать GPU, особенно при редукции и сканировании.

 - Настраивать количество потоков на блок в зависимости от объема данных и архитектуры GPU.

 - Использовать разделяемую память для ускорения операций внутри блока.

 - Применять рекурсивный или многоуровневый алгоритм Blelloch Scan для массивов, превышающих размер блока.
__________________________________________________________________________________________________________________________________

# Дополнительные задания: Реализовать алгоритм Blelloch Scan (Доп.задание2.ipynb)

Алгоритм Blelloch Scan — это эффективный параллельный метод вычисления префиксных сумм.
Он состоит из двух фаз:

 - Upsweep (reduce-фаза) — строится дерево сумм внутри блока.

 - Downsweep — формируются окончательные префиксные суммы.

Для обработки больших массивов применяется многоуровневый (рекурсивный) скан:

 - каждый блок вычисляет локальный scan,

 - суммы блоков сохраняются в отдельный массив,

 - этот массив рекурсивно сканируется,

 - затем суммы предыдущих блоков добавляются к результату.

Такой подход обеспечивает корректную работу для массивов любого размера.

Реализация

В программе реализованы:

 - blockScanKernel() — выполняет Blelloch scan внутри одного блока.

 - blellochScan() — рекурсивно запускает сканирование массива сумм блоков.

 - addBlockSums() — добавляет суммы предыдущих блоков к результатам.

 - CPU-версия последовательного сканирования для проверки и сравнения.

Используется shared memory для ускорения доступа внутри блока.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile practice74.cu
[код программы]

 Компилируем
!nvcc practice74.cu -o practice74 -arch=sm_75 -std=c++11    

 Запускаем
!./practice74

<img width="840" height="575" alt="image" src="https://github.com/user-attachments/assets/d7a8e908-6527-4d21-8024-98ad5be20acd" />

# Анализ результатов:

Реализация алгоритма Blelloch Scan показала значительное ускорение вычисления префиксной суммы на GPU по сравнению с последовательной CPU-версией.

Для массива из 1024 элементов время CPU составляет 0.005538 мс, а GPU — 0.12496 мс. На этом небольшом массиве GPU чуть медленнее из-за накладных расходов на запуск CUDA и синхронизацию потоков, что типично для малых данных.

На массивах в 1 000 000 элементов GPU значительно выигрывает: 0.646688 мс против 4.70519 мс у CPU. Параллельная обработка блоков позволяет выполнять префиксную сумму почти в 7 раз быстрее, чем последовательный вариант.

Для массива в 10 000 000 элементов GPU сохраняет высокую эффективность: 4.89974 мс против 46.9139 мс у CPU, что показывает почти десятикратное ускорение. Алгоритм Blelloch Scan хорошо масштабируется для больших данных.

Сравнение CPU и GPU:

 - CPU выполняет последовательное сканирование, время растет линейно с размером массива.

 - GPU использует параллельные блоки и разделяемую память, что ускоряет вычисления.

 - Для больших массивов GPU обеспечивает значительное ускорение, почти в 10 раз по сравнению с CPU.

 - Для малых массивов GPU немного уступает CPU из-за накладных расходов на запуск ядра и синхронизацию потоков.

Рекомендации по оптимизации:

 - Использовать GPU для массивов размером от сотен тысяч элементов и больше.

 - Применять многоуровневый (рекурсивный) Blelloch Scan для массивов, превышающих размер блока.

 - Настраивать количество потоков на блок для оптимальной загрузки GPU.

 - Использовать разделяемую память для ускорения вычислений внутри блока.

________________________________________________________________________________________________________________________________________________

# Дополнительные задания 3: Исследование влияния размера блока на производительность (Доп.задание3.ipynb)

Цель работы

Исследовать, как размер блока CUDA (threadsPerBlock) влияет на время выполнения параллельных алгоритмов:

 - редукции (reduction),

 - сканирования (scan),

и определить оптимальный размер блока.

Для каждого размера массива выполнялись измерения при размерах блока:

128 потоков

256 потоков

512 потоков

1024 потока

Измерялось время:

CPU редукции,

GPU редукции,

CPU сканирования,

GPU сканирования.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile practice75.cu
[код программы]

 Компилируем
!nvcc practice75.cu -o practice75 -arch=sm_75 -std=c++11    

 Запускаем
!./practice75

<img width="668" height="894" alt="image" src="https://github.com/user-attachments/assets/353db748-d998-4424-8504-41610c1cccad" />

# Анализ результатов: 

1. Малые массивы (1024 элемента):

 - На небольших данных выбор количества потоков на блок сильно влияет на производительность GPU.

 - С ростом числа потоков (от 128 до 512) время редукции и сканирования уменьшается, так как больше потоков могут обрабатывать данные параллельно.

 - При максимальном числе потоков на блок (1024) наблюдается небольшое увеличение времени, вероятно из-за накладных расходов на синхронизацию и управление большим числом потоков внутри блока.

 - CPU остаётся почти стабильным, так как последовательное выполнение не зависит от блоков.

2. Средние массивы (1 000 000 элементов):

 - GPU время редукции и сканирования минимально при 256 потоках на блок.

 - При меньшем числе потоков (128) GPU использует больше блоков, что увеличивает количество синхронизаций между ними.

 - При больших блоках (512 и 1024) накладные расходы на синхронизацию внутри блока растут, и выигрыш уменьшается.

 - CPU время растёт линейно с размером массива, не зависит от числа потоков.

3. Большие массивы (10 000 000 элементов):

 - На больших данных оптимальный размер блока — 256 потоков: GPU достигает минимального времени.

 - Слишком маленький блок (128) увеличивает количество блоков и дополнительных сумм для рекурсивного скана, что слегка увеличивает время.

 - Слишком большой блок (512–1024) приводит к росту накладных расходов на синхронизацию и управление потоками.

 - GPU по-прежнему показывает значительное ускорение по сравнению с CPU, почти в 10–15 раз быстрее.

Выводы:

 - Размер блока критически влияет на производительность GPU: есть оптимальный диапазон, который зависит от архитектуры устройства и размера данных.

 - Для малых массивов GPU может проигрывать CPU из-за накладных расходов на запуск ядра.

 - Для больших массивов оптимальные блоки — 256–512 потоков, обеспечивающие баланс между параллелизмом и накладными расходами на синхронизацию.

 - CPU остаётся стабильным, но GPU масштабируется лучше с ростом массива.
