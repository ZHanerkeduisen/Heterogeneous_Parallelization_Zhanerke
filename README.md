Zhanerke Duisen, ADA - 2403M

# Assignment 4

Блок-схемы в файле "БЛОК-СХЕМЫ"

Ответы на контрольные вопросы в файле "Контрольные вопросы assignment4.docx"

Описание: 
Цель задания — изучение и практическое применение CUDA и MPI для параллельных и распределённых вычислений на массиве данных. В рамках задания реализованы следующие задачи:

 - Сумма элементов массива на CPU и GPU (CUDA, глобальная память)

 - Префиксная сумма (scan) на CPU и GPU (CUDA, shared memory)

 - Гибридная обработка массива на CPU и GPU

 - Распределённая обработка массива с использованием MPI

Для всех заданий измерялось время выполнения и сравнивалась эффективность разных подходов.

________________________________________________________________________________________________________________________

# Задание 1 — Сумма массива с использованием CUDA (глобальная память) файл: Assignment4_Task1.ipynb

Описание задачи:

 - Вычислить сумму элементов массива на CPU и на GPU, используя глобальную память CUDA.

 - Сравнить результаты и время выполнения для массива 100 000 элементов.

Реализация:

 - CUDA kernel использует атомарное сложение (atomicAdd) для безопасного суммирования элементов в глобальной памяти.

 - CPU реализация — простой последовательный цикл.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile assignment41.cu
[код программы]

 Компилируем
!nvcc assignment41.cu -o assignment41 -arch=sm_75 -std=c++11    

 Запускаем
!./assignment41

<img width="1280" height="363" alt="image" src="https://github.com/user-attachments/assets/5d28fbf5-b423-4d31-8849-2dad3c373b87" />


Вывод: GPU выполняет вычисления быстрее за счёт параллельной обработки.

___________________________________________________________________________________________________________________________________________

# Задание 2 — Префиксная сумма (scan) с использованием CUDA (shared memory) файл: Assignment4_Task2.ipynb

Описание задачи:

Вычислить префиксную сумму массива на CPU и на GPU, используя shared memory CUDA.

Размер массива — 1 000 000 элементов.

Реализация:

 - Параллельный алгоритм префиксной суммы.

 - CPU реализация — последовательное накопление суммы.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile assignment42.cu
[код программы]

 Компилируем
!nvcc assignment42.cu -o assignment42 -arch=sm_75 -std=c++11    

 Запускаем
!./assignment42

<img width="1233" height="422" alt="image" src="https://github.com/user-attachments/assets/ddcad562-2b96-4a82-ba88-f7610715c04a" />


Выводы:

 - Префиксная сумма на GPU значительно ускоряет обработку больших массивов.

 - Shared memory позволяет эффективно хранить промежуточные значения в блоке.

__________________________________________________________________________________________________________________________________________

# Задание 3 — Гибридная обработка массива (CPU + GPU) файл: Assignment4_Task3.ipynb

Описание задачи:

 - Массив делится на две части: первую обрабатывает CPU, вторую — GPU.

 - Сравнить время выполнения CPU-, GPU- и гибридной реализации.

Реализация:

 - CPU вычисляет сумму первой половины массива.

 - GPU kernel вычисляет сумму второй половины массива.

 - Результаты объединяются в CPU после вычислений.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile assignment43.cu
[код программы]

 Компилируем
!nvcc assignment43.cu -o assignment43 -arch=sm_75 -std=c++11    

 Запускаем
!./assignment43

<img width="1280" height="419" alt="image" src="https://github.com/user-attachments/assets/33dc314e-89e2-43b8-9606-1dd6bf575964" />

Выводы:

 - Гибридная модель позволяет использовать ресурсы CPU и GPU одновременно.

 - В Colab GPU остаётся наиболее быстрым вариантом для больших массивов, но гибрид даёт прирост при совместном использовании CPU.

______________________________________________________________________________________________________________________________________________

# Задание 4 — Распределённая обработка массива с MPI

Описание задачи:

 - Реализовать распределённую программу для суммирования массива через MPI.

 - Массив делится между процессами, каждый процесс вычисляет локальную сумму, затем результаты собираются с помощью MPI_Reduce.

 - Проверка времени выполнения для 2, 4 и 8 процессов.

Реализация:

 - MPI_Scatter — распределяет массив между процессами.

 - Каждый процесс суммирует свою часть массива.

 - MPI_Reduce — собирает локальные суммы на процессе 0.

# Компиляция и запуск в Google Colab:

 Сохраняем программу в файл
%%writefile assignment44.cpp
[код программы]

 Компилируем
!mpic++ assignment44.cpp -o assignment44

 Запускаем с 2 процессами
!mpirun --allow-run-as-root --oversubscribe -np 2 ./assignment44

 Запускаем с 4 процессами
!mpirun --allow-run-as-root --oversubscribe -np 4 ./assignment44

 Запускаем с 8 процессами
!mpirun --allow-run-as-root --oversubscribe -np 8 ./assignment44


<img width="1275" height="606" alt="image" src="https://github.com/user-attachments/assets/8f4479d2-9be6-44fd-b551-54aad0da3a74" />

Выводы:

 - MPI корректно распределяет массив между процессами и собирает результаты.

 - Время выполнения уменьшается с увеличением числа процессов, но ускорение ограничено числом доступных CPU.

 - Для демонстрации параллельной обработки даже 2–4 процесса дают наглядный эффект.
