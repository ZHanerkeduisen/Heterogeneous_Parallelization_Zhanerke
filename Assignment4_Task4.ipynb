{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJF1bAc253Qi",
        "outputId": "f5ff915c-b0ef-4381-d184-cc2c5c8e7fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assignment44.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile assignment44.cpp\n",
        "// Assignment 4 Task 4\n",
        "// Реализовать распределённую программу с использованием MPI\n",
        "// Разделить массив между процессами, выполнить вычисления локально и собрать результаты\n",
        "// Замеры времени выполнения для 2, 4 и 8 процессов.\n",
        "#include <iostream>                               // Для стандартный ввод-вывод\n",
        "#include <mpi.h>                                  // MPI библиотека для распределённых вычислений\n",
        "#include <vector>                                 // Для динамического массива\n",
        "using namespace std;                              // Чтобы не писать std::\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);                        // Инициализация MPI среды\n",
        "\n",
        "    int world_rank;                                // Ранг текущего процесса\n",
        "    int world_size;                                // Общее количество процессов\n",
        "\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);    // Получаем ранг текущего процесса\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);    // Получаем общее количество процессов\n",
        "\n",
        "    const int N = 1000000;                         // Размер массива\n",
        "    vector<int> array;                             // Массив для хранения данных\n",
        "\n",
        "    if (world_rank == 0) {                         // Только процесс с рангом 0 инициализирует массив\n",
        "        array.resize(N);                           // Выделяем память под массив\n",
        "        for (int i = 0; i < N; i++)                // Заполняем массив единицами\n",
        "            array[i] = 1;\n",
        "    }\n",
        "\n",
        "    int local_size = N / world_size;               // Размер части массива для каждого процесса\n",
        "    vector<int> local_array(local_size);           // Локальный массив для каждого процесса\n",
        "\n",
        "    // Разделяем массив между процессами\n",
        "    MPI_Scatter(array.data(), local_size, MPI_INT, // Отправляем по частям\n",
        "                local_array.data(), local_size, MPI_INT,                        // Буфер приёма на каждом процессе (куда MPI положит свою часть)\n",
        "                                                                                // Количество элементов, которые каждый процесс примет\n",
        "                                                                                // Тип данных приёма (должен совпадать с отправкой)\n",
        "                0, MPI_COMM_WORLD);                                             // Ранг процесса-отправителя (root) — здесь процесс 0\n",
        "                                                                                // Коммуникатор, в рамках которого выполняется scatter\n",
        "\n",
        "    // Измеряем время локальных вычислений\n",
        "    double start_time = MPI_Wtime();               // Старт таймера MPI\n",
        "\n",
        "    int local_sum = 0;                             // Локальная сумма на каждом процессе\n",
        "    for (int i = 0; i < local_size; i++)          // Проходим по локальной части\n",
        "        local_sum += local_array[i];              // Суммируем элементы\n",
        "\n",
        "    int global_sum = 0;                            // Переменная для хранения глобальной суммы\n",
        "\n",
        "    // Собираем результаты с всех процессов на процесс 0\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    double end_time = MPI_Wtime();                 // Остановка таймера\n",
        "\n",
        "    // Процесс 0 выводит результат\n",
        "    if (world_rank == 0) {\n",
        "        cout << \"Глобальная сумма: \" << global_sum << endl;\n",
        "        cout << \"Время выполнения с \" << world_size << \" процессами: \"\n",
        "             << (end_time - start_time) * 1000 << \" мс\" << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                                // Завершаем MPI\n",
        "    return 0;                                      // Завершаем\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Компиляция\n",
        "!mpic++ assignment44.cpp -o assignment44\n",
        "# mpirun — утилита, которая запускает MPI-программу на указанном числе процессов\n",
        "# --allow-run-as-root — разрешает запуск MPI от root (Colab запускает ядра как root)\n",
        "# --oversubscribe — игнорирует количество доступных виртуальных CPU, позволяя запускать больше процессов, чем физически есть\n",
        "# -np 2 — число процессов MPI (2 процесса)\n",
        "\n",
        "# Запуск с 2 процессами\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./assignment44\n",
        "\n",
        "# Запуск с 4 процессами\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./assignment44\n",
        "\n",
        "# Запуск с 8 процессами\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./assignment44\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40jbieGT6SlV",
        "outputId": "215b44d5-2c61-4c24-d0c1-1c488ea10f9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Глобальная сумма: 1000000\n",
            "Время выполнения с 2 процессами: 2.45628 мс\n",
            "Глобальная сумма: 1000000\n",
            "Время выполнения с 4 процессами: 1.41423 мс\n",
            "Глобальная сумма: 1000000\n",
            "Время выполнения с 8 процессами: 0.6472 мс\n"
          ]
        }
      ]
    }
  ]
}